{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6acd55d-65d2-43cf-b590-95d75892142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"OpenAI.env\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# DIR = \"output\"\n",
    "DIR = \"output_DataScience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3404686a-24d8-4955-a87f-5543f05fb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFFICULTY_PROFILES = {\n",
    "    \"Beginner\": (\n",
    "        \"- Audience: newcomers; avoid jargon or define on first use.\\n\"\n",
    "        \"- Scaffolding: strong. Use short sentences, step-by-step guidance.\\n\"\n",
    "        \"- Examples: add simple, concrete examples and analogies.\\n\"\n",
    "        \"- Explanations: define acronyms and terms inline.\\n\"\n",
    "        \"- Tone: friendly, clear, encouraging.\\n\"\n",
    "    ),\n",
    "    \"Intermediate\": (\n",
    "        \"- Audience: practitioners; moderate jargon with quick clarifications.\\n\"\n",
    "        \"- Scaffolding: medium. Focus on practical tips and workflows.\\n\"\n",
    "        \"- Examples: show realistic scenarios and trade-offs.\\n\"\n",
    "        \"- Explanations: assume basics known; clarify non-obvious parts.\\n\"\n",
    "        \"- Tone: professional, concise.\\n\"\n",
    "    ),\n",
    "    \"Advanced\": (\n",
    "        \"- Audience: experts; domain terminology without basic definitions.\\n\"\n",
    "        \"- Scaffolding: light. Emphasize trade-offs, assumptions, edge cases.\\n\"\n",
    "        \"- Examples: deep dives, performance and limitation notes.\\n\"\n",
    "        \"- Explanations: focus on rigor, references, comparatives.\\n\"\n",
    "        \"- Tone: precise, analytical.\\n\"\n",
    "    ),\n",
    "    \"Executive\": (\n",
    "        \"- Audience: non-technical decision-makers.\\n\"\n",
    "        \"- Focus: business outcomes, risk, cost, ROI, timelines.\\n\"\n",
    "        \"- Scaffolding: narrative summaries, KPIs, options & implications.\\n\"\n",
    "        \"- Explanations: minimal technical detail; clear decisions needed.\\n\"\n",
    "        \"- Tone: crisp, outcome-oriented.\\n\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a42d3-3db2-49c7-82d0-1a6d259e6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_agent_currucula import CurriculaCourseAgent\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "\n",
    "client = OpenAI()  \n",
    "agent = CurriculaCourseAgent(client, \"gpt-5-nano\")\n",
    "\n",
    "# âœ… ĞĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ™ Ğ†ĞĞŸĞ£Ğ¢: ĞĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "course_name = \"AML Uncovered\"\n",
    "level = \"Intermediate\"\n",
    "total_hours = 12.0\n",
    "course_description = \"\"\"The â€œAML Uncovered â€“ Foundation Courseâ€ provides a comprehensive introduction to Anti-Money Laundering (AML) and Countering the Financing of Terrorism (CFT) principles. \n",
    "                        The course covers key concepts and regulatory frameworks, the stages of money laundering, international standards and the role of the FATF, \n",
    "                        customer due diligence (KYC) requirements, transaction monitoring, identification of suspicious activities, \n",
    "                        and the responsibilities of financial institution staff in detecting, reporting, and preventing financial crimes. \n",
    "                        It equips participants with a solid foundation for further professional development in compliance, financial integrity, and risk management.\n",
    "\n",
    "                        \n",
    "                        \"\"\"\n",
    "\n",
    "learning_objs = {\n",
    "    \"Compliance with AML Laws and Regulations\": 3,\n",
    "    \"Risk Assessment for Financial Transactions\": 3,\n",
    "    \"Suspicious Activity Reporting\": 3,\n",
    "    \"Transaction Monitoring Systems (e.g., Actimize)\": 3,\n",
    "    \"Anti-Money Laundering Investigations\": 4\n",
    "}\n",
    "modules_count = 6\n",
    "videos_per_module = 4\n",
    "readings_per_module = 1\n",
    "case_studies = True\n",
    "num_case_questions = 4 if case_studies else 0  # ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ Ğ´Ğ»Ñ ĞºĞµĞ¹ÑÑ–Ğ², ÑĞºÑ‰Ğ¾ case_studies=True\n",
    "quizzes = True\n",
    "exam_num_quiz_questions = 30 if quizzes else 0  # ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ Ğ´Ğ»Ñ ĞºĞ²Ñ–Ğ·Ñƒ, ÑĞºÑ‰Ğ¾ quizzes=True\n",
    "docx_file_path = \"New Microsoft Word Document.docx\"\n",
    "\n",
    "# âœ… ĞĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "exam_format = {\n",
    "    \"case_studies\": case_studies,\n",
    "    \"num_case_questions\": num_case_questions,\n",
    "    \"quizzes\": quizzes,\n",
    "    \"num_quiz_questions\": exam_num_quiz_questions,\n",
    "    \"passing_threshold\": \"70%\"\n",
    "}\n",
    "\n",
    "# âœ… Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ½Ğ¾Ğº Ñ‡Ğ°ÑÑƒ Ğ´Ğ»Ñ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "exam_time_minutes = (num_case_questions * 10) + (exam_num_quiz_questions * 2)  # 10 Ñ…Ğ²/ĞºĞµĞ¹Ñ, 2 Ñ…Ğ²/Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ ĞºĞ²Ñ–Ğ·Ñƒ\n",
    "exam_time_hours = exam_time_minutes / 60.0\n",
    "# print(f\"â° Estimated exam time: {exam_time_minutes} minutes ({exam_time_hours:.2f} hours)\")\n",
    "\n",
    "# Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ†Ğ¯\n",
    "print(f\"ğŸ¤– Building course {course_name}...\")\n",
    "structure = agent.generate_course_structure(\n",
    "    course_name, level, total_hours, course_description,\n",
    "    modules_count, videos_per_module, readings_per_module,\n",
    "    case_studies, num_case_questions, quizzes, exam_format, learning_objs,\n",
    "    docx_file_path,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# Ğ’Ğ˜Ğ’Ğ†Ğ”\n",
    "agent.print_professional_outline(structure)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "# Generate the filename\n",
    "filename = f\"{course_name.replace(' ', '_')}_curriculum.json\"\n",
    "\n",
    "# Create the full path by joining the output directory and filename\n",
    "full_path = os.path.join(DIR, filename)\n",
    " \n",
    "# Save the JSON file\n",
    "with open(full_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(structure, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Build JSON: {full_path}\")\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = elapsed % 60\n",
    "\n",
    "print(f\"Elapsed time: {minutes} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0a141b-a1dd-46ef-a9be-4646491898af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Building course DataScience...\n",
      "Word loadad: DataScience.docx\n",
      "\n",
      "GENERATING COURSE â€” attempt 1/3\n",
      "ERROR: Quiz question: 14 â‰  2\n",
      "Attempt 1 failed. Retrying...\n",
      "\n",
      "GENERATING COURSE â€” attempt 2/3\n",
      "Validation passed successfully!\n",
      "SUCCESS on attempt 2!\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "COURSE DataScience | Beginner | 360 MINUTES (6.0 hours)\n",
      "Description: DataScience is a practical, entryâ€‘level course designed to build confident foundational skills in data-driven discovery. It guides learners through the complete data science workflow, from collecting and cleaning data to visualizing results and building simple models. You will engage with handsâ€‘on programming, visualization, statistical reasoning, and modern data tools to develop practical competencies. Through guided exercises and realâ€‘world examples, youâ€™ll gain stepâ€‘byâ€‘step experience in identifying problems, preparing data, and communicating insights to stakeholders. The course emphasizes applying concepts to real projects and fosters a mindset of iterative learning and experimentation. By the end, youâ€™ll have a solid base to pursue introductory data science topics and contribute to dataâ€‘driven projects. Itâ€™s ideal for aspiring data scientists, analysts, students, and professionals seeking a practical entry point. Expect to finish with ready-to-use skills and a clear path toward more advanced topics.\n",
      "LEARNING OBJECTIVES (Bloom's Taxonomy Level):\n",
      "   â€¢ Introduction & Programming Basics â†’ Level 1 â˜…â˜†â˜†â˜†â˜†\n",
      "   â€¢ Data Exploration & Visualization â†’ Level 2 â˜…â˜…â˜†â˜†â˜†\n",
      "   â€¢ Machine Learning & Applications â†’ Level 3 â˜…â˜…â˜…â˜†â˜†\n",
      "Exam Duration: 100 minutes\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "Module 1: Module 1: Introduction & Programming Basics\n",
      "  Videos (3):\n",
      "    - Video 1: What is Data Science? (8 min): This introductory video defines data science and situates it within business and research contexts. It explains the end-to-end workflow, from data collection to communicating insights, with concrete examples. Youâ€™ll see how data problems are framed, the roles of data scientists, and the value of an iterative approach. Real-world contexts illustrate how data science informs decisions, improves processes, and supports outcomes. Practical tips help you start thinking like a data scientist from the first lesson.\n",
      "    - Video 2: The Data Science Workflow (7 min): This video breaks down the data science workflow into manageable stages: problem definition, data collection, cleaning, exploration, modeling, evaluation, and storytelling. It covers how to translate questions into data tasks and how to plan analyses. The narrative emphasizes reproducibility, versioning, and collaboration as core practices. It also highlights common pitfalls and how to avoid them through simple, structured processes. Youâ€™ll see how each step connects to practical outcomes and project milestones.\n",
      "    - Video 3: Python Essentials for Data Science (7 min): In this video youâ€™ll learn essential Python fundamentals for data work, including data structures, control flow, and simple functions. The session demonstrates how Python enables data manipulation, quick analyses, and rapid experimentation. Examples feature common libraries used in data science workflows and how to run small, repeatable scripts. The goal is to build confidence with Python basics to support hands-on practice in later modules.\n",
      "  Readings (1):\n",
      "    - Reading: Foundations of Data Handling (28 min): This reading introduces data types, data sources, and data quality concepts. It explains how data is collected, stored, and accessed, and why data cleaning is a critical first step in any analysis. The material covers basic data manipulation operations and the importance of reproducible workflows. It provides practical examples of common data issues such as missing values, outliers, and inconsistent formatting. Key takeaways include aligning data preparation with analysis goals and documenting assumptions for future analyses. The reading also outlines a simple data pipeline concept to frame subsequent modules.\n",
      "  Case Studies (1):\n",
      "    - Case Study 1: A Practical Data Problem (40 min): This case study presents a small dataset and a realâ€‘world problem that requires framing, data collection, cleaning, and initial exploration. You will identify the data sources, assess data quality, and outline a simple approach to obtaining initial insights. The scenario emphasizes clear problem statements, stakeholder needs, and the role of data in informing decisions. It provides a structured environment to practice translating a business question into data tasks. The narrative invites you to consider ethical and privacy considerations in data handling and to propose a highâ€‘level plan for analysis.\n",
      "      Learning Outcomes: Identify data sources and data types involved in a simple problem, Explain the importance of data cleaning and preparation, Outline a basic data pipeline for a small dataset, Summarize initial insights and potential next steps\n",
      "      Questions (4): What is the core data science problem described in the case?, Which data sources are involved and what types of data do they contain?, What is the first data cleaning step you would apply and why?, What initial insights could be shared with stakeholders based on the data?\n",
      "  Quiz (7 questions):\n",
      "    - Which statement best describes the data science workflow? (2 min)\n",
      "    - What is the primary purpose of data cleaning? (2 min)\n",
      "    - Which language is commonly used for data science tasks covered in this module? (2 min)\n",
      "    - What is a key benefit of documenting assumptions in a data project? (2 min)\n",
      "    - Which stage comes earliest in the data science workflow? (2 min)\n",
      "    - Which is a common risk when working with data? (2 min)\n",
      "    - A simple pipeline for a dataset typically includes which of the following? (2 min)\n",
      "    Quiz Total Duration: 14 min\n",
      "  Module Total Duration: 104 min\n",
      "\n",
      "Module 2: Module 2: Data Exploration & Visualization\n",
      "  Videos (3):\n",
      "    - Video 1: Data Cleaning Essentials (8 min): This video highlights practical data cleaning techniques, including handling missing values, correcting data formats, and validating data quality. Youâ€™ll see examples of simple rules to detect anomalies and ensure consistency across datasets. The session emphasizes how clean data enables reliable analyses and better visualizations. Realistic scenarios show how to document cleaning steps for reproducibility. The goal is to build a toolkit you can apply to diverse datasets from the start.\n",
      "    - Video 2: Exploratory Data Analysis (EDA) (7 min): In this video youâ€™ll learn how to summarize and explore data to reveal patterns and relationships. Topics include distribution checks, correlations, and identifying outliers. The session demonstrates practical approaches to formulating hypotheses and validating them with quick visual checks. Youâ€™ll see how EDA informs feature selection and modeling decisions. Concrete examples illustrate how to iterate from questions to insights.\n",
      "    - Video 3: Visualization Basics (6 min): This video covers the basics of effective data visualization, including choosing appropriate chart types and storytelling with visuals. Youâ€™ll learn common pitfalls to avoid and techniques to enhance clarity and interpretability. The session includes examples of dashboards and simple visual narratives you can apply to real datasets. Practical tips help you communicate results to diverse audiences. The aim is to equip you with visuals that support actionable insights.\n",
      "  Readings (1):\n",
      "    - Reading: Intro to Statistics for Data Science (28 min): This reading introduces core statistical ideas essential for data exploration, including measures of central tendency, variability, and basic distributions. It explains how to interpret summary statistics and how they relate to data visualization. Concepts such as p-values and confidence intervals are discussed at a beginner level to build intuition. The material also covers the role of sampling and the importance of avoiding overfitting in early analyses. Practical examples link statistical ideas to real datasets, enabling you to apply what you learn directly. The reading concludes with guidance on using statistics to inform decisions and storytelling.\n",
      "  Case Studies (1):\n",
      "    - Case Study 2: Clean, Explore, and Visualize (40 min): This case study walks through a dataset requiring cleaning, exploratory analysis, and visualization to uncover trends. Youâ€™ll identify data quality issues, apply basic cleaning steps, and perform visual explorations to reveal patterns. The scenario emphasizes choosing appropriate visual representations to communicate insights. It also highlights how EDA informs next steps and feature considerations for simple models. Throughout, youâ€™ll practice documenting your process and presenting findings clearly.\n",
      "      Learning Outcomes: Explain the purpose of data cleaning in preparation for exploration, Apply basic exploratory data analysis techniques to identify patterns, Select appropriate visualization methods to communicate findings, Summarize insights and potential actions based on data patterns\n",
      "      Questions (4): What cleaning steps would you apply to this dataset and why?, Which variable shows the strongest relationship with the target and how would you interpret it?, Which visualization best reveals a potential trend in the data and why?, What are two potential data quality issues you should address before modeling?\n",
      "  Quiz (7 questions):\n",
      "    - Which visualization is generally best for comparing categories? (2 min)\n",
      "    - What is a key purpose of exploratory data analysis? (2 min)\n",
      "    - Which measure describes the center of a numerical distribution? (2 min)\n",
      "    - Outliers primarily affect which aspect of a dataset? (2 min)\n",
      "    - Which plot helps identify relationships between two numerical variables? (2 min)\n",
      "    - Which term describes the spread of a dataset? (2 min)\n",
      "    - What is the main goal of data visualization in analytics? (2 min)\n",
      "    Quiz Total Duration: 14 min\n",
      "  Module Total Duration: 103 min\n",
      "\n",
      "Module 3: Module 3: Machine Learning & Applications\n",
      "  Videos (3):\n",
      "    - Video 1: ML Fundamentals (9 min): This video introduces core machine learning concepts, including supervised vs. unsupervised learning, model training, and evaluation. It explains how to frame a predictive task, select a simple modelling approach, and interpret results. Practical examples show how data preparation influences model performance. The session emphasizes intuition and caution to avoid common pitfalls like overfitting. By the end, youâ€™ll understand the basic workflow for applying ML to small problems.\n",
      "    - Video 2: Regression & Classification (8 min): In this session youâ€™ll explore two foundational modelling tasks: regression for predicting continuous values and classification for categorizing outcomes. Youâ€™ll see how to split data, train simple models, and evaluate performance with common metrics. The video includes practical tips for feature selection and model interpretation. Examples demonstrate how model choices affect results and stakeholder communications. The focus is on applying simple ML concepts to real datasets.\n",
      "    - Video 3: Advanced ML Overview (6 min): This overview presents a broader view of machine learning applications, including model selection tradeâ€‘offs, feature engineering ideas, and practical deployment considerations. It highlights the importance of evaluating models on unseen data and communicating results responsibly. Realâ€‘world examples illustrate how ML can be used for decision support, automation, and insight generation. The aim is to give you a highâ€‘level sense of how ML fits into broader data projects and strategies.\n",
      "  Readings (1):\n",
      "    - Reading: Introduction to Machine Learning Concepts (28 min): This reading covers foundational ML ideas, including the difference between training and testing, common algorithms, and the importance of data quality. It explains performance metrics for regression and classification and provides beginner-friendly examples. The material also discusses model interpretation and communicating results to non-technical stakeholders. Practical tips help you think critically about when to apply ML and how to avoid common missteps. The reading closes with a short roadmap for progressing from basics to more advanced topics.\n",
      "  Case Studies (1):\n",
      "    - Case Study 3: Building a Simple Predictive Model (40 min): This case study guides you through building a straightforward predictive model on a small dataset. Youâ€™ll define the problem, prepare data, select a simple algorithm, and evaluate performance. The scenario emphasizes understanding feature importance and communicating results clearly. It also highlights the iterative nature of model development and the need for careful validation. Youâ€™ll practice documenting steps and justifying modelling choices for stakeholders.\n",
      "      Learning Outcomes: Explain how a simple predictive model is built and evaluated, Identify features that influence predictions and justify their inclusion, Communicate model results and limitations to a non-technical audience, Demonstrate an iterative approach to improving a model\n",
      "      Questions (4): What is the target variable in the case study and why is it chosen?, Which features are most impactful and how would you justify their inclusion?, What evaluation metric is most appropriate for this task and why?, What limitations should be communicated to stakeholders about the model?\n",
      "  Quiz (7 questions):\n",
      "    - Which type of learning uses labeled data for prediction? (2 min)\n",
      "    - What is a common metric for regression tasks? (2 min)\n",
      "    - Which task best describes classification? (2 min)\n",
      "    - Which step helps prevent overfitting? (2 min)\n",
      "    - What is feature engineering? (2 min)\n",
      "    - Which concept relates to how well a model generalizes? (2 min)\n",
      "    - Which statement about model interpretation is true? (2 min)\n",
      "    Quiz Total Duration: 14 min\n",
      "  Module Total Duration: 105 min\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "FINAL EXAM:\n",
      "  Case Studies (1):\n",
      "    - Exam Case Study: Predicting House Prices (40 min): This exam case study challenges you to apply data science literacy to a predictive modeling task. Youâ€™ll outline a structured approach, identify relevant features, and discuss model selection and evaluation. The scenario emphasizes clear justification of choices and communication of results. You will consider data quality issues and potential biases that could affect the interpretation of outcomes. The case is designed to test your ability to translate knowledge into a practical, defensible solution under exam conditions.\n",
      "      Learning Outcomes: Explain a structured approach to a predictive modeling problem, Identify relevant features and justify their inclusion, Describe evaluation strategies for model performance, Discuss data quality and bias considerations in a real-world task\n",
      "      Questions (4): What is the primary objective of the house prices prediction task and how would you frame it for modeling?, Which features would you consider first and why might they be predictive?, What evaluation metric would you choose and why is it appropriate for this task?, What data quality issues could affect the results and how would you address them?\n",
      "  Quiz (39 questions):\n",
      "    - Which practice improves model generalization to new data? (2 min)\n",
      "    - What does feature engineering aim to achieve? (2 min)\n",
      "    - Which metric is commonly used for regression tasks? (2 min)\n",
      "    - Why is data leakage problematic? (2 min)\n",
      "    - Which of the following is a visualization suitable for regression residuals? (2 min)\n",
      "    - What is a key consideration when communicating ML results to stakeholders? (2 min)\n",
      "    - Which approach helps detect data quality issues early? (2 min)\n",
      "    - Which statement best reflects responsible ML practice? (2 min)\n",
      "    - Which step is typically performed before model selection? (2 min)\n",
      "    - What is a common cause of biased predictions in ML models? (2 min)\n",
      "    - Which technique helps with model interpretability? (2 min)\n",
      "    - Which scenario best demonstrates validation on unseen data? (2 min)\n",
      "    - What is the primary goal of model evaluation? (2 min)\n",
      "    - Which scenario indicates potential overfitting? (2 min)\n",
      "    - Which of the following is a recommended practice for reporting ML results? (2 min)\n",
      "    - Which data preparation step is often critical before modeling? (2 min)\n",
      "    - Which metric is appropriate for binary classification evaluation? (2 min)\n",
      "    - What is the purpose of a train-test split? (2 min)\n",
      "    - Which approach can help prevent leakage in time-series data? (2 min)\n",
      "    - Which statement about model deployment is true? (2 min)\n",
      "    - Which task is often part of postâ€‘deployment monitoring? (2 min)\n",
      "    - Which technique helps address class imbalance? (2 min)\n",
      "    - What is a key benefit of model explainability? (2 min)\n",
      "    - Which factor is essential when choosing a modeling approach? (2 min)\n",
      "    - Which statement best describes a baseline model? (2 min)\n",
      "    - Which of the following helps with reproducibility? (2 min)\n",
      "    - What is a common pitfall in ML projects? (2 min)\n",
      "    - What is the role of a validation set? (2 min)\n",
      "    - Which practice supports responsible AI? (2 min)\n",
      "    - Which is a good practice when presenting ML results? (2 min)\n",
      "    - Which term describes the process of updating a model with new data? (2 min)\n",
      "    - Which evaluation metric is often used for imbalanced classification problems? (2 min)\n",
      "    - What does model drift refer to? (2 min)\n",
      "    - Which is a reason to perform crossâ€‘validation? (2 min)\n",
      "    - Which statement best describes a confusion matrix? (2 min)\n",
      "    - What is the purpose of feature scaling in ML? (2 min)\n",
      "    - Which practice helps verify that your model is usable in production? (2 min)\n",
      "    - Which approach is recommended for handling missing values in features? (2 min)\n",
      "    - What is a key takeaway from ML applications in this course? (2 min)\n",
      "    Quiz Total Duration: 78 min\n",
      "  Passing Threshold: 70%\n",
      "  Exam Total Duration: 118 min\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "Total Course Duration (Modules + Exam): 430 min\n",
      "Expected Total Duration (from input): 360 min\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ’¾ Build JSON: output_DataScience/DataScience_curriculum.json\n",
      "Elapsed time: 4 min 47.84 sec\n"
     ]
    }
   ],
   "source": [
    "from AI_agent_currucula import CurriculaCourseAgent\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "\n",
    "client = OpenAI()  \n",
    "agent = CurriculaCourseAgent(client, \"gpt-5-nano\")\n",
    "\n",
    "# âœ… ĞĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ™ Ğ†ĞĞŸĞ£Ğ¢: ĞĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "course_name = \"DataScience\"\n",
    "level = \"Beginner\"\n",
    "total_hours = 6.0\n",
    "course_description = \"\"\"This beginner-level course introduces learners to the essential concepts, tools, and techniques in data science. \n",
    "                        You will explore the complete data science workflowâ€”from collecting and analyzing data to building basic machine learning models and communicating insights. \n",
    "                        Through practical examples and guided exercises, youâ€™ll gain hands-on experience with programming, visualization, statistical reasoning, and modern data tools. \n",
    "                        By the end of the course, youâ€™ll have the foundational knowledge to pursue advanced data science topics and real-world projects with confidence.\n",
    "                        \"\"\"\n",
    "\n",
    "learning_objs = {\n",
    "    \"Introduction & Programming Basics\": 1,\n",
    "    \"Data Exploration & Visualization\": 2,\n",
    "    \"Machine Learning & Applications\": 3\n",
    "}\n",
    "modules_count = 3\n",
    "videos_per_module = 3\n",
    "readings_per_module = 1\n",
    "case_studies = True\n",
    "num_case_questions = 4 if case_studies else 0  # ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ Ğ´Ğ»Ñ ĞºĞµĞ¹ÑÑ–Ğ², ÑĞºÑ‰Ğ¾ case_studies=True\n",
    "quizzes = True\n",
    "exam_num_quiz_questions = 30 if quizzes else 0  # ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ Ğ´Ğ»Ñ ĞºĞ²Ñ–Ğ·Ñƒ, ÑĞºÑ‰Ğ¾ quizzes=True\n",
    "docx_file_path = \"DataScience.docx\"\n",
    "\n",
    "# âœ… ĞĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "exam_format = {\n",
    "    \"case_studies\": case_studies,\n",
    "    \"num_case_questions\": num_case_questions,\n",
    "    \"quizzes\": quizzes,\n",
    "    \"num_quiz_questions\": exam_num_quiz_questions,\n",
    "    \"passing_threshold\": \"70%\"\n",
    "}\n",
    "\n",
    "# âœ… Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ½Ğ¾Ğº Ñ‡Ğ°ÑÑƒ Ğ´Ğ»Ñ Ñ–ÑĞ¿Ğ¸Ñ‚Ñƒ\n",
    "exam_time_minutes = (num_case_questions * 10) + (exam_num_quiz_questions * 2)  # 10 Ñ…Ğ²/ĞºĞµĞ¹Ñ, 2 Ñ…Ğ²/Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ ĞºĞ²Ñ–Ğ·Ñƒ\n",
    "exam_time_hours = exam_time_minutes / 60.0\n",
    "# print(f\"â° Estimated exam time: {exam_time_minutes} minutes ({exam_time_hours:.2f} hours)\")\n",
    "\n",
    "# Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ†Ğ¯\n",
    "print(f\"ğŸ¤– Building course {course_name}...\")\n",
    "structure = agent.generate_course_structure(\n",
    "    course_name, level, total_hours, course_description,\n",
    "    modules_count, videos_per_module, readings_per_module,\n",
    "    case_studies, num_case_questions, quizzes, exam_format, learning_objs,\n",
    "    docx_file_path,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# Ğ’Ğ˜Ğ’Ğ†Ğ”\n",
    "agent.print_professional_outline(structure)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "# Generate the filename\n",
    "filename = f\"{course_name.replace(' ', '_')}_curriculum.json\"\n",
    "\n",
    "# Create the full path by joining the output directory and filename\n",
    "full_path = os.path.join(DIR, filename)\n",
    " \n",
    "# Save the JSON file\n",
    "with open(full_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(structure, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Build JSON: {full_path}\")\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = elapsed % 60\n",
    "\n",
    "print(f\"Elapsed time: {minutes} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934ff7e8-683f-404c-b75b-735c7a421120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DOCX CREATED: output_DataScience/DataScience_curriculum.docx\n",
      "ğŸ‰ DONE! Open: output_DataScience/DataScience_curriculum.docx\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "json_file = full_path\n",
    "docx_file = os.path.splitext(full_path)[0] + \".docx\"\n",
    "\n",
    "agent.create_course_docx(json_file, docx_file)\n",
    "print(f\"ğŸ‰ DONE! Open: {docx_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65eeedb-590e-4978-8b00-a1125f299cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
